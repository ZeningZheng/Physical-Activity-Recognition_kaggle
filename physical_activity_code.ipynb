{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **1.0 Library & File Preparation**","metadata":{}},{"cell_type":"code","source":"library(MASS)\nlibrary(tidyverse)\nsuppressMessages(library(caret)) \nlist.files(path = \"../input\")","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:58:17.50781Z","iopub.execute_input":"2021-10-01T09:58:17.509725Z","iopub.status.idle":"2021-10-01T09:58:17.531758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure the data is available\nif (length(list.files(\"../input\", pattern = \"recognition\")) > 0) {\n    \n    # Copy all files to the current directory\n    system(\"cp -r ../input/bda-2021-physical-activity-recognition/* ./\")\n    \n} else {\n    \n    # Download data for this competition\n    data_url = \"https://phonesensordata.netlify.app/Archive.zip\"\n    download.file(data_url, \"Archive.zip\")\n\n    # Unzip all files in the current directory\n    unzip(\"Archive.zip\")\n    \n}\n\n# list files in the current working directory\nlist.files()\n\n# show the content of the labels file \nfile.show(\"activity_labels.txt\")","metadata":{"_uuid":"c6073784c036efe22d6d5cc50d5e941dd039e7ee","execution":{"iopub.status.busy":"2021-10-01T09:58:17.556443Z","iopub.execute_input":"2021-10-01T09:58:17.55813Z","iopub.status.idle":"2021-10-01T09:58:18.026579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The signals themselves are stored in text files. In these files there are three columns; each column is the signal measured in one of the 3 channels of the sensor (these channels are associated in X, Y and Z direction). Each signal consists of a sequence of measurements, called _samples_.\n","metadata":{"_uuid":"1683fc0df5ce55aa0e3a50be13d0cba17c2675f6"}},{"cell_type":"markdown","source":"## **2.0 Label Preperation**\nMake each sample labeled with an activity.","metadata":{"_uuid":"6781f50084a24a9e05cc6b4240b63352177a3ee7"}},{"cell_type":"code","source":"# Import label\nact_labels = read_delim(\"activity_labels.txt\",\" \",col_names=F,trim_ws=T) \nact_labels = act_labels %>% select(X1,X2)\n\nlabels = read_delim(\"./RawData/Train/labels_train.txt\", \" \", col_names = F)\ncolnames(labels) <- c('trial', 'userid', 'activity', 'start', 'end')\n\nlabels = labels %>% mutate(activity = act_labels$X2[activity])\n\n# Add the sequence start:end to each row in a list.\nsample_labels_nested = \n    labels %>% \n    rowwise() %>% # do next operation(s) rowwise\n    mutate(sampleid = list(start:end)) %>%\n    ungroup()\n\n\n# Unnest the nested tabel.\nsample_labels = \n    sample_labels_nested %>% \n\n    # Rows are segments, we need to keep track of different segements\n    mutate(segment = row_number() ) %>% \n\n    # Expand the data frame to one sample per row\n    unnest() %>% \n\n    # Remove columns we don't need anymore\n    select(-start, -end) \n\n\n# Check the result (first few rows are not interesting; rows 977-990 are)\nprint(sample_labels[977:990, ])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:58:18.030253Z","iopub.execute_input":"2021-10-01T09:58:18.031776Z","iopub.status.idle":"2021-10-01T09:58:18.204967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.0 **Feature Extraction from Signals**\n\nSeveral features can be extracted from signals. We extracted the following:\n\n### Statistical features:\n* **Power:** represents the average squared amplitude of the signal.\n* **Energy:** the “total power” of the n samples.\n* **Entropy:** a measure of average surprise in handling signals: ‘Surprise’ about an observed value can be expressed in terms of the negative logarithm of the probability of that outcome; the higher the probability, the less ‘surprised’ you are.\n* **Mean**\n* **Median**\n* **Min and max**\n* **Mean absolute deviation**\n* **Correlation**\n\n### Time domain features:\n* **Cosine angle:** the scaled inner product. This time domain feature takes the ordering of the sequence of numbers into account.\n* **Lagged correlation**\n\n### Frequency domain features:\n* **Mean frequency:** the sum of the product of the spectrogram intensity (in dB) and the frequency, divided by the total sum of spectrogram intensity.\n* **Standard deviation:** a measure for how much the frequencies in a spectrum can deviate from the centre of gravity.\n* **Spectrum peak**\n* **Skewness:** a measure for how much the shape of the spectrum below the centre of gravity is different from the shape above the mean frequency.\n* **Kurtosis:** provides a measure of the “peakedness” of a random signal.","metadata":{}},{"cell_type":"markdown","source":"### Feature choice explanation\n\nWe used the data from all measurements for our predictors (so X1, X2 and X3). We chose all our predictors because they seemed to vary strongly across the different activities. \n\n* From the histograms plotted it follows that both the mean and standard deviation from X1, X2 and X3 are very different for a lot of activities. This is for example clear when looking at X1 in the histograms for ‘laying’ and ‘walking upstairs’. In ‘walking upstairs’ the mean of X1 is evidently a lot higher. \n* Another predictor we chose is the autocorrelation of the subsequent measurements of the same variable (lagged correlation). A high autocorrelation occurs when a predictor doesn’t differ a lot among measurements as is for example visible in the histogram for ‘standing’. In this histogram neither of the measurements X1, X2 and X3 seem to vary a lot, contrary to a lot of the other histograms. \n* We also included skewness (how symmetric the distribution of the histogram is) and kurtosis (how much the histogram is ‘spread out) as predictors. These statistics also vary greatly across the different activities. The distribution of X1 is for example quite symmetrical for ‘walking upstairs’ and is very flat (low kurtosis) for ‘lie to stand’, so these measures should be suitable to distinguish among the different activities. \n* Lastly we included entropy as a predictor for the activity because it seems that for some activities the measurements seem to follow a certain pattern and are a bit more predictable, while in other they are more chaotic.","metadata":{}},{"cell_type":"markdown","source":"### Feature functions","metadata":{}},{"cell_type":"code","source":"# Helper functions\nmost_common_value = function(x) {\n    counts = table(x, useNA = 'no')\n    most_frequent = which.max(counts)\n    return(names(most_frequent))\n}\n\n## Feature functions\nlagged_cor = function(x, y = x, lag = 0) {\n    # compute correlation between x and a time shifted y\n    r_lagged = cor(x, dplyr::lag(y, lag), use = 'pairwise')\n    return(r_lagged)\n}\n\n\n# entropy\nentropy  <- function(x, nbreaks = nclass.Sturges(x)) {\n    r = range(x)\n    x_binned = findInterval(x, seq(r[1], r[2], len = nbreaks))\n    h = tabulate(x_binned, nbins = nbreaks) # fast histogram\n    p = h/sum(h)\n    -sum(p[p>0] * log(p[p>0]))\n}\n\n\n# spectrum_peak (code from group 10 and 3)\nspectrum_peak <- function(x){\n    spec = spectrum(x, log = 'n', plot = FALSE)$spec\n    entropy = entropy(spec)\n    return(entropy)\n}\n\n\n#spectrum_mean (code from group 10 and 3)\nspectrum_mean <- function(x) {\n    \n    spec = spectrum(x, log = 'n', plot = FALSE)$spec\n    freq = spectrum(x, log = 'n', plot = FALSE)$freq\n    df = freq[2] - freq[1]\n    \n    spec_mean <- sum(freq * spec * df)\n    \n    if(is.na(spec_mean)) {\n        return(0)\n    } else {\n        return(spec_mean)\n    }\n}\n\n#spectrum_sd (code from group 10 and 3)\nspectrum_sd <- function(x) {\n    spec = spectrum(x, log = 'n', plot = FALSE)$spec\n    freq = spectrum(x, log = 'n', plot = FALSE)$freq\n    df = freq[2] - freq[1]\n    \n    return(sqrt(sum((freq - mean(x))^2 * spec * df)))\n}\n","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:58:18.208749Z","iopub.execute_input":"2021-10-01T09:58:18.210342Z","iopub.status.idle":"2021-10-01T09:58:18.236847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function of putting things all together\n","metadata":{"_uuid":"70f82939d0025cd90b72ea9b46b23c9a110f4f38"}},{"cell_type":"code","source":"extractTimeDomainFeatures <- function(filename, sample_labels) {\n    \n    # extract user and experimental run ID's from file name\n    username = gsub(\".+user(\\\\d+).+\", \"\\\\1\", filename) %>% as.numeric()\n    expname  = gsub( \".+exp(\\\\d+).+\", \"\\\\1\", filename) %>% as.numeric()\n    \n    # import the sensor signals from the file\n    user01 <- read_delim(filename, \" \", col_names = F, progress = TRUE, \n                 col_types = \"ddd\")\n    \n    \n    # merge signals with labels \n    user_df <- \n        data.frame(userid = username, trial = expname, user01) %>%\n        mutate(sampleid = 0:(nrow(user01)-1) ) %>%\n        left_join(sample_labels, by = c('userid','trial','sampleid')) \n\n    \n    # split in epochs of 128 samples and compute features per epoch\n    usertimedom <-  user_df %>%\n    \n          # add an epoch ID variable (on epoch = 2.56 sec)\n          mutate(epoch = sampleid %/% 128) %>% \n\n          # extract statistical features from each epoch\n          group_by(epoch) %>%\n          summarise(\n            # keep track of user and experiment information\n            user_id = username, \n            exp_id = expname,   \n              \n            # epoch's activity labels and start sample\n            activity = most_common_value(c(\"-\", activity)),\n            sampleid = sampleid[1],\n              \n            # features\n            \n            # mean\n            m1 = mean(X1), \n            m2 = mean(X2),\n            m3 = mean(X3),\n            \n            # median\n            median1 = median(X1),\n            median2 = median(X2),\n            median3 = median(X3),\n            \n            # min and max\n            min1 = min(X1),\n            min2 = min(X2),\n            min3 = min(X3),\n            max1 = max(X1),\n            max3 = max(X2),\n            max3 = max(X3),\n              \n            # mean absolute deviation (from group 10)\n            mad1 = mad(X1), \n            mad2 = mad(X2),   \n            mad3 = mad(X3),\n           \n            # variation\n            var1 = var(X1),\n            var2 = var(X2),\n            var3 = var(X3),\n              \n            # standard deviation\n            sd1 = sd(X1), \n            sd2 = sd(X2), \n            sd3 = sd(X3),              \n\n            \n            # lagged correlation\n            AR1_1 = lagged_cor(X1, lag = 1),\n            AR2_1 = lagged_cor(X1, lag = 1),\n            AR3_1 = lagged_cor(X1, lag = 1),\n            AR12_1 = lagged_cor(X1, X2, lag = 1),\n            AR13_1 = lagged_cor(X1, X3, lag = 1),\n            AR23_1 = lagged_cor(X2, X3, lag = 1),\n\n              \n            # correlation  \n            cor1_2 = cor(X1, X2),\n            cor1_3 = cor(X1, X3), \n            cor2_3 = cor(X2, X3),\n              \n            \n            # entropy\n            entropy1 = entropy(X1),\n            entropy2 = entropy(X2),\n            entropy3 = entropy(X3),\n              \n            # power\n            power1 = mean(X1^2),\n            power2 = mean(X2^2),\n            power3 = mean(X3^2),\n            \n            # quartiles\n            q1_25 = quantile(X1, .25),\n            q2_25 = quantile(X2, .25),\n            q3_25 = quantile(X3, .25),\n            q1_75 = quantile(X1, .75),\n            q2_75 = quantile(X2, .75),\n            q3_75 = quantile(X3, .75),\n              \n            # skewness\n            skew1 = e1071::skewness(X1),\n            skew2 = e1071::skewness(X2),\n            skew3 = e1071::skewness(X3), \n            \n            # kurtosis\n            kurt1 = e1071::kurtosis(X1),\n            kurt2 = e1071::kurtosis(X2),\n            kurt3 = e1071::kurtosis(X3),\n              \n            # cosine angle (code from group 10)\n            cosangle1 = X1 %*% X2 / sqrt(sum(X1^2) * sum(X2^2)), \n            cosangle2 = X1 %*% X3 / sqrt(sum(X1^2) * sum(X3^2)),  \n            cosangle3 = X3 %*% X2 / sqrt(sum(X3^2) * sum(X2^2)),\n            n_samples = n(),\n             \n            # spectrum features (code from group 10 and 3)\n            spectrum1 = spectrum_mean(X1), \n            spectrum2 = spectrum_mean(X2),\n            spectrum3 = spectrum_mean(X3),\n            spectrumsd1 = spectrum_sd(X1),   \n            spectrumsd2 = spectrum_sd(X2),\n            spectrumsd3 = spectrum_sd(X3),\n            spectrumpeak1 = spectrum_peak(X1),\n            spectrumpeak2 = spectrum_peak(X2),\n            spectrumpeak3 = spectrum_peak(X3) \n          ) \n    \n    usertimedom \n}","metadata":{"_uuid":"daccc2b4175580864f72a2012ef6d56ee7c0a883","execution":{"iopub.status.busy":"2021-10-01T09:58:18.239555Z","iopub.execute_input":"2021-10-01T09:58:18.241037Z","iopub.status.idle":"2021-10-01T09:58:18.254946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.0 **Import Training Data & Cleaning**\n\n## Import","metadata":{}},{"cell_type":"code","source":"# run this for all files of both the 'acc' and 'gyro' data\nfilenames <- dir(\"./RawData/Train/\", \"^acc\", full.names = TRUE) \nfilenames_gyro <- dir(\"./RawData/Train/\", \"^gyro\", full.names = TRUE) \n\n\nmyData = map_dfr(filenames, extractTimeDomainFeatures, sample_labels) \nmyData2 = map_dfr(filenames_gyro, extractTimeDomainFeatures, sample_labels)\n\n\nmyData <- myData %>%\nleft_join(myData2, by = c(\"user_id\", \"epoch\", \"exp_id\", \"activity\", \"sampleid\", \"n_samples\"), \n          suffix = c(\".acc\", \".gyro\"))\n\n\nhead(myData) \nglimpse(myData)","metadata":{"_uuid":"c2572d092b5b590a3f6dcc67313ff5f690beb471","execution":{"iopub.status.busy":"2021-10-01T09:58:18.257625Z","iopub.execute_input":"2021-10-01T09:58:18.259074Z","iopub.status.idle":"2021-10-01T10:00:04.255644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning\n\n- Remove unlabeled epochs\n- keep epochs defined in the sample submission file\n- Remove epochs that do not consist of 128 samples\n- Delete unuseful columns: epoch,user_id, exp_id, sampleid, n_samples\n- Finding non zero variation\n- Remove NA's\n- Remove Highly correlated variables\n\n","metadata":{"_uuid":"689e96162446707ee9e88a4c229d169f76290b62"}},{"cell_type":"markdown","source":"## Disclaimer","metadata":{}},{"cell_type":"markdown","source":"We only found out very last minute that the way in which we cleaned the data (see code below) was probably not right. Because we removed certain rows with no lables the order of our predictions did not match the order of the test set. Hence we think our score on the leaderboard is quite low, while the accuracy of our best model is almost 0.9. However trying to fix the data gave some new errors that we did not have time to fix last minute. ","metadata":{}},{"cell_type":"code","source":"## Data cleaning\n# Remove unlabeled epochs\nmyData_labeled <- subset(myData, epoch != \"-\")\n\n# Only keep epochs defined in the sample submission file\nmyData_labeled <- subset(myData, activity != \"-\")\n\n# Remove epochs that do not consist of 128 samples\nmyData_sampeled <- subset(myData_labeled, n_samples = 128)\n\n# Delete unuseful columns: epoch,user_id, exp_id, sampleid, n_samples\nmyData_useful <- subset(myData_sampeled, select = -c(epoch,user_id, exp_id, sampleid, n_samples))\n\n# Finding non zero variation\nnear_zero <- caret::nearZeroVar(myData_useful) # no non zero variation\n\n# Remove NA's\nany(is.na(myData_useful)) # no NA's\n\n# Highly correlated variables,remove them\nhigh_cor_var <- caret::findCorrelation(cor(myData_useful[,-c(1)])) # remove activity column to make it numeric\nhigh_cor_var_correct <- high_cor_var + 1\n\nmyData_clean <- myData_useful[,-high_cor_var_correct]\n\nhead(myData_clean)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:04.259385Z","iopub.execute_input":"2021-10-01T10:00:04.261011Z","iopub.status.idle":"2021-10-01T10:00:09.274039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5.0 Model Fitting**\n- Logistic regression\n- LDA\n- QDA\n- k-NN","metadata":{}},{"cell_type":"code","source":"# cross validation\ntrcntr = trainControl('cv', number = 10, p=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:09.276691Z","iopub.execute_input":"2021-10-01T10:00:09.278256Z","iopub.status.idle":"2021-10-01T10:00:09.291212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"#fit_multinom = caret::train(activity ~ ., data = myData_clean, method=\"multinom\", trControl = trainControl('cv', number = 2, p=0.8))\n\n#fit_multinom","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:09.293784Z","iopub.execute_input":"2021-10-01T10:00:09.295293Z","iopub.status.idle":"2021-10-01T10:00:09.305952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LDA: Linear Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"## LDA Model\nfit_lda = caret::train(activity ~ ., data = myData_clean, method=\"lda\", trControl = trcntr)\n\nfit_lda","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:09.308512Z","iopub.execute_input":"2021-10-01T10:00:09.310101Z","iopub.status.idle":"2021-10-01T10:00:12.481282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## QDA: Quadratic discriminant analysis","metadata":{}},{"cell_type":"markdown","source":"We ran this model but it took a very long time and had poor accuracy so we decided to comment it out.","metadata":{}},{"cell_type":"code","source":"## QDA Model \n\n#fit_qda = caret::train(activity ~ ., data = myData_clean, method=\"qda\", trControl = trcntr)\n\n#fit_qda","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:12.483708Z","iopub.execute_input":"2021-10-01T10:00:12.48505Z","iopub.status.idle":"2021-10-01T10:00:12.495579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN: k-nearest neighbors","metadata":{}},{"cell_type":"code","source":"## k-Nearest Neighbours Model\nfit_knn = caret::train(activity ~ ., data = myData_clean, method=\"knn\", trControl = trcntr)\n\nfit_knn","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:12.49795Z","iopub.execute_input":"2021-10-01T10:00:12.4993Z","iopub.status.idle":"2021-10-01T10:00:22.026732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the end we used the scaled k-Nearest Neighbours Model for our predictions, because it has the highest accuracy as becomes clear from the barplot below. We also did include highly correltated predictors in the model, because this improved the accuracy even more, eventhough is makes the model more complex.  ","metadata":{}},{"cell_type":"code","source":"## Scaled k-Nearest Neighbours Model\nfit_knns = caret::train(activity ~ .,\n               data = myData_clean, method=\"knn\", trControl = trcntr, preProcess = \"scale\")\nfit_knns_all = caret::train(activity ~ .,\n               data = myData_useful, method=\"knn\", trControl = trcntr, preProcess = \"scale\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:22.029077Z","iopub.execute_input":"2021-10-01T10:00:22.030437Z","iopub.status.idle":"2021-10-01T10:00:48.935989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6.0 Comparing Model Performances**\n\n# Performance visualization\n\nA simple and effective visualization of the performance accuracy is a `barplot`. ","metadata":{}},{"cell_type":"code","source":"## Visualize model performance differences\n\n\nmodels = list(lda = fit_lda, knn = fit_knn, knns = fit_knns, knns_full = fit_knns_all )\n\n# extract the cross-validated accuracies from each model\nAcc = sapply(models, function(mdl) max(mdl$results$Accuracy)) \n\n# make a barplot with only the best performing model in red\ncolor = 1 + (Acc >= max(Acc)) \nbarplot(Acc, horiz=T, las=1, col = color)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:48.938827Z","iopub.execute_input":"2021-10-01T10:00:48.940271Z","iopub.status.idle":"2021-10-01T10:00:49.027083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **7.0 Prediction**","metadata":{}},{"cell_type":"code","source":"# Import test data\n\nfilenames_test_acc <- dir(\"./RawData/Test/\", \"^acc\", full.names = TRUE)\nfilenames_test_gyro <- dir(\"./RawData/Test/\", \"^gyro\", full.names = TRUE) \n\ntest_data_acc = map_dfr(filenames_test_acc, extractTimeDomainFeatures, sample_labels) \ntest_data_gyro = map_dfr(filenames_test_gyro, extractTimeDomainFeatures, sample_labels)\n\n\ntest_data <- test_data_acc %>%\nleft_join(test_data_gyro, by = c(\"user_id\", \"epoch\", \"exp_id\", \"activity\", \"sampleid\", \"n_samples\"), \n          suffix = c(\".acc\", \".gyro\"))\n\nglimpse(test_data)\n\n# Predict\ntest_result <- predict(fit_knns_all, test_data)\n\ntest_data['activity'] <- test_result","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:49.029496Z","iopub.execute_input":"2021-10-01T10:00:49.030892Z","iopub.status.idle":"2021-10-01T10:01:34.007317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8.0 Submissions**","metadata":{"_uuid":"63f77917e08cd690655137184750cfac494607ce"}},{"cell_type":"code","source":"test_data %>%\n\n    # prepend \"user\" and \"exp\" to user_id and exp_id\n    mutate(\n        user_id = paste(ifelse(user_id < 10, \"user0\", \"user\"), user_id, sep = \"\"), \n        exp_id = paste(ifelse(exp_id < 10, \"exp0\", \"exp\"), exp_id, sep = \"\")\n    ) %>% \n\n    # unit columnes user_id, exp_id and sample_id into a string \n    # separated by \"_\" and store it in the new variable `Id`\n    unite(Id, user_id, exp_id, sampleid) %>%\n\n    # retain only the `Id` and  predictions\n    dplyr::select(Id, Predicted = activity) %>%\n\n    # write to file\n    write_csv(\"test_set_predictions.csv\")\n\n\n# Check the result: print first 20 lines in the submission file\ncat(readLines(\"test_set_predictions.csv\",20), sep = \"\\n\")","metadata":{"_uuid":"d7eb43a15f0e63a6a6e1c2474bda14e82094abb2","execution":{"iopub.status.busy":"2021-10-01T10:01:34.009768Z","iopub.execute_input":"2021-10-01T10:01:34.011184Z","iopub.status.idle":"2021-10-01T10:01:34.057553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Division of Labor:\n**Britt:**\n* Writing code for data cleaning\n* Writing code for model fitting\n* Code styling\n* Finding features and implementing\n\n**Zena:**\n* Model fitting, prediction\n* Find useful features\n* Tidy document, debug\n\n**Jesse:**\n* Adding the gyro data\n* Adding basic predictors and spectrum predictors\n* Explaining our choice of features\n* Explaining model selection","metadata":{}}]}